{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture3 Loss Functions and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cs231n.github.io/linear-classify/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents of Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## To Do\n",
    "    - **Loss Function**\n",
    "        - Def \n",
    "        - Usage\n",
    "        - \n",
    "    - **Optimization**\n",
    "        - Def \n",
    "            - find the parameters that minimize the loss function\n",
    "\n",
    "            - point of machine learning is not lies in fitting train data, but test data\n",
    "            - we usally solve this problem(overfit) by using regularization\n",
    "                -  add additional terms of loss function \n",
    "                    - Occam's Razor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Def\n",
    "    - Measures that quantifies unhappinesss of the model's result compare with the optimal choice\n",
    "        - how to quantify how bad are diffenrent mistake\n",
    "        - how to weight off the different trade-offs between different types of mistake the classifier might make\n",
    "- ### Usage\n",
    "    - Tell how good our current classifier is, given a dataset \n",
    "    - Use as objective function for optimization\n",
    "        - Associated with Econ's Utility function(negative)\n",
    "- ### Type\n",
    "    - #### Hinge Loss\n",
    "        - maximum-margin classification\n",
    "        - Max form function\n",
    "        - Care only whether the score of ture class is **much** higher **than** the score of other classes, not the score **itself**\n",
    "            - **much**\n",
    "                - more than the given margin\n",
    "            - **than**\n",
    "                - relatively higher\n",
    "                    - Difference between Softmax\n",
    "            - **itself**\n",
    "                - Cares absolute score\n",
    "                    - Softmax\n",
    "         - If score of true category is larger than the scroe of False catefories, than Loss is zero\n",
    "         - If the true class's score is less than the other score, than we get some loss\n",
    "        - #### Diff with Squared Hinge Loss\n",
    "            - if we use hinge loss\n",
    "                - we don't actually care between being a little bit wrong and being a lot wrong \n",
    "            - if we use squred hinge loss\n",
    "                - very bad things can be interpreted as squred very bad things \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "## params\n",
    "s_j = -0.5\n",
    "fixed_margin = 1\n",
    "\n",
    "s_y = np.linspace(-2, 2)\n",
    "L = [np.max([x, 0]) for x in (s_j - s_y + fixed_margin)]\n",
    "fig = plt.figure(figsize = (8, 4))\n",
    "plt.plot(s_y, L)\n",
    "plt.scatter(s_j, 0, c = \"r\")\n",
    "plt.plot([s_j, s_j + fixed_margin ], [0,0], c = \"r\")\n",
    "plt.scatter(s_j + 1, 0, c = \"r\")\n",
    "plt.text(s_j + 0.3, .1, f\"margin : {fixed_margin }\" )\n",
    "plt.title(\"Hinge Loss\")\n",
    "plt.xlabel(\"s_h\")\n",
    "plt.ylabel(\"L\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Regularization\n",
    "    - **Def**\n",
    "        - Anything that penalizes the complexity of the model\n",
    "            - rather than explicitly trying to fit the training data\n",
    "    - **Usage**\n",
    "        - usually we use L2 regulizer(weight decay)\n",
    "            - But choosing regulizer depends on how do you think that the complexity should be measured\n",
    "        - $\\lambda R(W)$terms help model to prefer regression model to be lower degree rather than higher degree\n",
    "     \n",
    "    - **Examples**\n",
    "        - **L2 regularizaiton **\n",
    "            - **Def** \n",
    "                - penalizing the euclidian norm of W\n",
    "                - weight decay\n",
    "                - measure the complexity of classifier\n",
    "            - **Equation**\n",
    "                - $R(W) \\sum\\sum W^2$\n",
    "            - **Mechanism**\n",
    "                - Deals with second momentum\n",
    "                    - Linear classification useally done with dot product.\n",
    "                    - However, L2 regularization deals with second momentum\n",
    "                        - which dot product does not measure\n",
    "                        - L2 norm can capture some situation that dot product omits\n",
    "                            - especially situation that [0.25, 0.25, 0.25, 0.25] and [0, 0, 0, 1]\n",
    "            - **See also**\n",
    "                - MAP inference using a Gaussian prior on W\n",
    "        - **L1 regularization**\n",
    "            - **Def**\n",
    "                - penalizing L1 norm of W\n",
    "                - Encourage sparse solutions\n",
    "                    - derives all your entries of W to zero\n",
    "                    - number of non-zero entries\n",
    "            - **Equation**\n",
    "                - $R(W) \\sum\\sum |W|$                  \n",
    "        - **Elastic net(L1 + L2)**\n",
    "            - $R(W) \\sum\\sum \\beta W^2 + |W|$                        \n",
    "        - **Maxnorm regularization**\n",
    "            - penalizing max norm rather than L1 and L2\n",
    "        - **Dropout**\n",
    "        - **Fancier**\n",
    "            - Batch normalization\n",
    "            stochastic depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 1. Multilcass SVM loss  \n",
    "    - Def\n",
    "        - Kinds of **Hinge Loss**\n",
    "    - Equation\n",
    "        - $L_i = \\sum max(s_j - s_{y_i} + 1,0)$\n",
    "    - Terms of notation\n",
    "        - $s$\n",
    "            - predicted scores come out of the classifier\n",
    "        - $s_{y_i}$\n",
    "            - score of ground truth label\n",
    "        - $s_j$\n",
    "            - Score of flase label\n",
    "    - Process\n",
    "        - For each class, we sub score of true class from the other classes.\n",
    "            - If value of true class is max in terms of all classes score in the class, than loss might be 0\n",
    "            - On the other hands, if true class socre is small than the other classes, than loss might be possitive\n",
    "    - Properties\n",
    "        - Uniqueness\n",
    "            - The set of W is not necessarily unique. \n",
    "            - if some parameters W is optimal parmameters, than so does $\\lambda W$\n",
    "            - In this reason we use regularization penalty $\\lambda R(W) $\n",
    "    - Q\n",
    "        - 1) Min, Max\n",
    "            - Min : 0\n",
    "            - Max : inf\n",
    "        - 2) Think of situation that W is small so all s = 0(usually initialization), what is the Loss?(Standard Initial Loss)\n",
    "            - In this case score follows uniform distribution\n",
    "            - result is #classes -1\n",
    "                - for each False class $max(s_j - s_{y_i} + 1,0)$, if s_j and s_{y_i} is 0, than $max(s_j - s_{y_i} + 1,0) = 1$\n",
    "                    - iterate c - 1 times\n",
    "        - 3) what happen if the add true class's score to sum of score?\n",
    "            - loss increses by 1, which make optimal loss as 1\n",
    "                - We psychologically want loss to be 0, that is why we omit true score when computing Loss function\n",
    "        - 4) What if we use mean instead of sum\n",
    "            - the answer does not change\n",
    "                - Linear Operation\n",
    "        - 5) What if we use $L_i = \\sum max(0, s_j - s_{y_i} + 1)^2$\n",
    "            - this is a different problem compare with previous problem.\n",
    "                - because we change loss function in non-leaiear way\n",
    "                - this kinds of squred hinge loss somtimes used in practice\n",
    "                    - Diff\n",
    "                        - setting loss function is same as saying computer to how you care about and what types of errors it should trade off against \n",
    "                        - setting linear loss function means you focuss more about the fact of difference itself\n",
    "                        - setting squared loss function means you focuss more quantity of difference \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_i_vectorized(x, y, W):\n",
    "    scores = W.dot(x)\n",
    "    margins = np.maximum(0, scores - scores[y] + 1)\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 2. Softmax classifier (Multinomial Logistic Regression) / Cross Entrophy\n",
    "    - Def\n",
    "        - Meaning of Score\n",
    "            - compute probability distribution over classes\n",
    "            - expoenentiate the score\n",
    "                - This process make score tobe positive\n",
    "            - normalize them by the sum fo those expoents\n",
    "                - range 0 to 1, and sum of score is 1 \n",
    "        - probabilities of the classes\n",
    "            - compare computed probability distribution with real probability distribution(which consist of 0 and 1)\n",
    "                - KL divergence\n",
    "                - MLE\n",
    "    - Goal\n",
    "        - probability of true class is high and as close to one\n",
    "        - loss : negative log of probabilities\n",
    "            - we want to maximize probability\n",
    "            - log is monotonic(does not ditrupt order) and make equation easy to compute\n",
    "                - therefore maximize log probability is the same problem with maximize probability\n",
    "            - Loss function is for computing badness\n",
    "                - therefore minimizing negative log probability is fit with the definition of loss function and our goal, maximizing true probability\n",
    "- Equation\n",
    "        - $L_i = -log{e^{s y_i} \\over \\sum e^{s_j}}$\n",
    "    - Q\n",
    "        - What is min and max of possible loss L_i\n",
    "            - Min : 0\n",
    "            - Max : inf\n",
    "        - W is small so all s = 0, than whaat is the loss\n",
    "            - $-log {1 \\over C} = log C$\n",
    "        - Suppose I take a datapoint and I jiggle a bit, whta happens to the loss in both case?\n",
    "            - SVM\n",
    "                - Does not change\n",
    "                - cares only the fact that whether correct score is greather than a margin above the incorrect scores\n",
    "            - Softmax\n",
    "                - loss get smaller as score of true class get higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recap\n",
    "    - We have Dataset and W\n",
    "    - Score Fucntion($f(x;W)$)\n",
    "    - Loss function\n",
    "        - Softmax\n",
    "        - SVM\n",
    "    - Full Loss\n",
    "        - ${1 \\over N}\\sum L_i + R(W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Def\n",
    "    - Walking Lage Vally\n",
    "- Examples\n",
    "    - Strategy #1 : Random Search\n",
    "        - Bad\n",
    "    - Strategy #2 : Follow the Slope\n",
    "        - What is Slope?\n",
    "            - 1-dimension\n",
    "                - derivative\n",
    "            - Multiple dimensions\n",
    "                - Gradient is the vector of along each dimension\n",
    "            - dot product of the direction with thje gradient\n",
    "                - steepes descent is the negative gradient\n",
    "        - Gradient Descent\n",
    "            - Calculating all gradient is inefficient\n",
    "                - but accuarte\n",
    "            - Use analytic gradient(With chain Rule)\n",
    "                - But implicate errores\n",
    "        - Stochastic Gradient Descent\n",
    "            - Use when N is too large to compute\n",
    "                - Approxiamte sum using a minibatch of examples\n",
    "                    - 32, 64, 128 is common\n",
    "            - rather than computing the loss and gradient over the entrie training set\n",
    "            - at every iteration we sample some small set\n",
    "                - mini-batch\n",
    "                    - use mini-batch to compute an estimate of the full sum\n",
    "                - may can see Monte Carlo estimate of some expectation                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-e1d8835fcf36>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e1d8835fcf36>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    return = weights_grad\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def evaluate_gradient(loss_func, data, weights):\n",
    "    return = weights_grad\n",
    "    \n",
    "while True:\n",
    "    weights_grad = evaluate_gradient(loss_func, data, weights)\n",
    "    weights += -step_size * weights_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gradient(loss_func, data, weights):\n",
    "    return = weights_grad\n",
    "    \n",
    "while True:\n",
    "    data_batch = sample_training_data(data,256)\n",
    "    weights_grad = evaluate_gradient(loss_func, data_batch, weights)\n",
    "    weights += -step_size * weights_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Image Features : Motivation\n",
    "    - Previous\n",
    "        - try to solve problem by using feature trainsform when cannot separate data with linear classifier\n",
    "        - Gistogram of Oriented Gradients\n",
    "            - find dominent edge of image \n",
    "        - Image -> Feature Extraction -> F - > Training\n",
    "    - CNN\n",
    "        - Image -< Training\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tip\n",
    "step size is the first hyper-parameter that I always check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
